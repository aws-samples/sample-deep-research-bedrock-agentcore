"""Research tools for Chat Agent to access research findings from AgentCore Memory"""

import os
import json
import logging
from typing import Optional
import boto3
from botocore.exceptions import ClientError
from strands import tool

logger = logging.getLogger(__name__)


class ResearchTools:
    """
    Class-based tools for accessing research findings from AgentCore Memory.

    These tools allow the chat agent to read detailed research content
    that was generated by the research agent and stored in AgentCore Memory.
    """

    def __init__(self, research_session_id: str, research_memory_id: str, user_id: str,
                 cached_events: Optional[list] = None, aws_region: str = 'us-west-2'):
        """
        Initialize research tools with session context

        Args:
            research_session_id: The research session ID to access
            research_memory_id: AgentCore Memory ID where research events are stored
            user_id: User ID (actor ID used by Research Agent)
            cached_events: Pre-loaded events from research context (prevents re-querying)
            aws_region: AWS region for AgentCore Memory client
        """
        self.research_session_id = research_session_id
        self.research_memory_id = research_memory_id
        self.user_id = user_id
        self.aws_region = aws_region

        # Create shared AgentCore Memory client
        self.client = boto3.client('bedrock-agentcore', region_name=aws_region)

        # Use pre-loaded events if available (prevents ThrottledException)
        self._events_cache: Optional[list] = None
        if cached_events:
            logger.info(f"ResearchTools using {len(cached_events)} pre-loaded events (from cache)")
            self._parse_and_cache_events(cached_events)
        else:
            logger.info(f"ResearchTools initialized for session {research_session_id} (will load on demand)")

    def _parse_and_cache_events(self, raw_events: list):
        """
        Parse and cache raw events from AgentCore Memory

        Args:
            raw_events: List of raw event objects from AgentCore Memory
        """
        parsed_events = []
        for event in raw_events:
            try:
                payload = event.get('payload', [])
                if isinstance(payload, list) and len(payload) > 0:
                    blob_data = payload[0].get('blob')
                    if blob_data:
                        data = json.loads(str(blob_data))
                        parsed_events.append({
                            'event_id': event.get('eventId'),
                            'type': data.get('event_type'),
                            'data': data
                        })
            except Exception as e:
                logger.debug(f"Error parsing event: {e}")
                continue

        self._events_cache = parsed_events
        logger.info(f"Cached {len(parsed_events)} parsed events")

    def _load_events(self) -> list:
        """
        Load all research events from AgentCore Memory (with caching)

        Returns:
            List of parsed event dictionaries
        """
        if self._events_cache is not None:
            return self._events_cache

        try:
            logger.info(f"Loading research events for session {self.research_session_id}")

            all_events = []
            next_token = None

            # Paginate through all events
            while True:
                params = {
                    'memoryId': self.research_memory_id,
                    'sessionId': self.research_session_id,
                    'actorId': self.user_id,  # Use actual user_id (not 'default_user')
                    'includePayloads': True,
                    'maxResults': 100
                }

                if next_token:
                    params['nextToken'] = next_token

                response = self.client.list_events(**params)

                # Parse events
                for event in response.get('events', []):
                    try:
                        payload = event.get('payload', [])
                        if isinstance(payload, list) and len(payload) > 0:
                            blob_data = payload[0].get('blob')
                            if blob_data:
                                data = json.loads(str(blob_data))
                                all_events.append({
                                    'event_id': event.get('eventId'),
                                    'type': data.get('event_type'),
                                    'data': data
                                })
                    except Exception as e:
                        logger.debug(f"Error parsing event: {e}")
                        continue

                next_token = response.get('nextToken')
                if not next_token:
                    break

            logger.info(f"Loaded {len(all_events)} research events")
            self._events_cache = all_events
            return all_events

        except ClientError as e:
            logger.error(f"AWS error loading events: {e}")
            return []
        except Exception as e:
            logger.error(f"Error loading events: {e}")
            return []

    @tool
    def read_aspect_research(self, dimension: str, aspect: str) -> str:
        """
        Read detailed research findings for a specific aspect.

        Args:
            dimension: Exact name of the dimension (case-sensitive, check research context for exact names)
            aspect: Exact name of the aspect within the dimension (case-sensitive)

        Returns comprehensive research content (typically 1000-2000 words) with inline citations.
        """
        try:
            logger.info(f"Reading aspect research: {dimension} / {aspect}")

            events = self._load_events()

            # Find matching aspect research event
            for event in events:
                if event['type'] == 'aspect_research_complete':
                    data = event['data']

                    if (data.get('dimension') == dimension and
                        data.get('aspect') == aspect):

                        # Extract research content
                        research_content = data.get('research_content', {})
                        content = research_content.get('content', '')
                        word_count = research_content.get('word_count', 0)
                        citations_count = data.get('citations_count', 0)

                        result = f"""# {dimension} - {aspect}

{content}

---
*Research: {word_count:,} words, {citations_count} citations*
"""
                        logger.info(f"✅ Found aspect research: {word_count} words")
                        return result

            return f"""❌ No research found for dimension '{dimension}', aspect '{aspect}'.

Please check the research context for exact dimension and aspect names.
Make sure the names match exactly (case-sensitive)."""

        except Exception as e:
            logger.error(f"Error reading aspect research: {e}")
            return f"❌ Error: {str(e)}"


def create_research_tools(research_session_id: str, research_memory_id: str, user_id: str,
                         cached_events: Optional[list] = None, aws_region: str = 'us-west-2') -> list:
    """
    Factory function to create research tools for a specific research session

    Args:
        research_session_id: Research session ID to access
        research_memory_id: AgentCore Memory ID where research events are stored
        user_id: User ID (actor ID used by Research Agent)
        cached_events: Pre-loaded events from research context (prevents re-querying Memory)
        aws_region: AWS region for AgentCore Memory client

    Returns:
        List of tool methods ready to be passed to Strands Agent
    """
    tools = ResearchTools(research_session_id, research_memory_id, user_id, cached_events, aws_region)

    return [
        tools.read_aspect_research,
    ]
